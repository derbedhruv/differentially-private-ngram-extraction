{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadf4e17-726f-461f-9448-1e45a712830a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this section if you want to replicate results from the paper for the reddit case\n",
    "## WARNING: These are large files and so this step can take some time.\n",
    "## If you want to run experiments on your own dataset, skip this cell and move to the next one\n",
    "\n",
    "# First, download the reddit file\n",
    "! curl -L -O https://zenodo.org/record/1043504/files/corpus-webis-tldr-17.zip\n",
    "\n",
    "# Unzip it\n",
    "! unzip corpus-webis-tldr-17.zip\n",
    "\n",
    "# Convert it to the required format\n",
    "! python scripts/convert_reddit.py --input_path corpus-webis-tldr-17.json --output_path reddit.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91d81959-70c6-4327-98e6-12b9c71fb1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# location of source file to extract DPNE from\n",
    "# MAKE SURE TO UPDATE THIS TO POINT TO YOUR DESIRED DATASET IF YOU DIDN'T RUN THE PREVIOUS CELL\n",
    "SOURCE_DATASET=\"./reddit.json\"\n",
    "\n",
    "# output folder high level\n",
    "OUTPUT_FOLDER=\"output\"\n",
    "\n",
    "# file extension - json or anything else.\n",
    "FILE_EXTENSION=\"json\"\n",
    "\n",
    "# epsilon for DP\n",
    "DP_EPSILON=\"4.0\"\n",
    "\n",
    "# the highest N in n-grams to DP-extract\n",
    "NGRAM_SIZE_LIMIT=\"10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92c2f441-1e72-431a-be2d-ee77d4b912fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run shell scripts to have an easy interface to experiment and get started with DPNE\n",
    "# This script runs a series of tokenization, ngram extraction, and DP N-gram extraction using the parameters specified as arguments\n",
    "## NOTE: the --persist-flags argument for the extract DPNE step (third line below) was set to 00 for local running, but you may want to change it back to the default 11 value to persist intermediate results!\n",
    "\n",
    "! spark-submit dpne/tokenize_text.py -f json --ngrams {NGRAM_SIZE_LIMIT} --max_num_tokens 400 --allow_multiple_ngrams 1 -i {SOURCE_DATASET} -o ./{OUTPUT_FOLDER}/tokenize_text -t {FILE_EXTENSION}\n",
    "\n",
    "! spark-submit dpne/split_ngrams.py --ngram_size {NGRAM_SIZE_LIMIT} -i ./$OUTPUT_FOLDER/tokenize_text -o ./{OUTPUT_FOLDER}/split_ngrams -f {FILE_EXTENSION} -t {FILE_EXTENSION}\n",
    "\n",
    "! spark-submit dpne/extract_dpne.py --dp_epsilon {DP_EPSILON} --dp_eta 0.1 --dp_delta 0.5 --contribution_limit 10 --persist_flags 00 --log_flags 00 --top_k 1 --delta_user_count 0 --ngram_size {NGRAM_SIZE_LIMIT} --filter_one_side 0 --budget_distribute 10.0 --estimate_sample_size 0.8 -i ./{OUTPUT_FOLDER}/split_ngrams -o ./{OUTPUT_FOLDER}/dpne_sample -f {FILE_EXTENSION} -t {FILE_EXTENSION}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f868084b-5780-493f-8137-359221f1f05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats on 1gram\n",
      "Stats on 2gram\n",
      "Stats on 3gram\n",
      "Stats on 4gram\n"
     ]
    }
   ],
   "source": [
    "# analyze and plot the resultant data\n",
    "import os, sys, pandas as pd\n",
    "\n",
    "try:\n",
    "    ngrams_folder = os.listdir(\"./{OUTPUT_FOLDER}/dpne_sample\".format(OUTPUT_FOLDER=OUTPUT_FOLDER))\n",
    "except:\n",
    "    print(\"Something went wrong in writing the ngrams in the previous step. Please double check\")\n",
    "\n",
    "DPNGRAMS = {} # will map string \"Ngram\" => pandas DataFrame containing those N-grams\n",
    "ngrams_folder.sort()\n",
    "for ngram in ngrams_folder:\n",
    "    # print stats of each ngram discovered\n",
    "    print(\"Stats on\", ngram)\n",
    "    for partfile in os.listdir(\"./{OUTPUT_FOLDER}/dpne_sample/{ngram}\".format(OUTPUT_FOLDER=OUTPUT_FOLDER, ngram=ngram)):\n",
    "        partfile_split = partfile.split(\".\")\n",
    "        if (len(partfile_split) == 2 and partfile_split[1] == \"json\"):\n",
    "            with open(os.path.join(\"./{OUTPUT_FOLDER}/dpne_sample/{ngram}\".format(OUTPUT_FOLDER=OUTPUT_FOLDER, ngram=ngram), partfile), 'r') as f:\n",
    "                DPNGRAMS[ngram] = pd.read_json(f, orient='records', lines=True)\n",
    "                display(DPNGRAMS[ngram])\n",
    "# Now you can use the appropriate dataframe for further investigation    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc79470-7ddd-4ba2-8b69-459b21869267",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
